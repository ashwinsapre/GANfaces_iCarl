{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMc6FTYGig3XeoJHbQ80cFk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinsapre/GANfaces_iCarl/blob/main/dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjLb4Rx45EGu"
      },
      "source": [
        "import os\r\n",
        "import time\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "from glob import glob\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "from PIL import Image\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwZ650ct5MRp"
      },
      "source": [
        "def generator(z, output_channel_dim, training):\r\n",
        "    with tf.variable_scope(\"generator\", reuse= not training):\r\n",
        "        \r\n",
        "        # 8x8x1024\r\n",
        "        fully_connected = tf.layers.dense(z, 8*8*1024)\r\n",
        "        fully_connected = tf.reshape(fully_connected, (-1, 8, 8, 1024))\r\n",
        "        fully_connected = tf.nn.leaky_relu(fully_connected)\r\n",
        "\r\n",
        "        # 8x8x1024 -> 16x16x512\r\n",
        "        trans_conv1 = tf.layers.conv2d_transpose(inputs=fully_connected,\r\n",
        "                                                 filters=512,\r\n",
        "                                                 kernel_size=[5,5],\r\n",
        "                                                 strides=[2,2],\r\n",
        "                                                 padding=\"SAME\",\r\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                                 name=\"trans_conv1\")\r\n",
        "        batch_trans_conv1 = tf.layers.batch_normalization(inputs = trans_conv1,\r\n",
        "                                                          training=training,\r\n",
        "                                                          epsilon=EPSILON,\r\n",
        "                                                          name=\"batch_trans_conv1\")\r\n",
        "        trans_conv1_out = tf.nn.leaky_relu(batch_trans_conv1,\r\n",
        "                                           name=\"trans_conv1_out\")\r\n",
        "        \r\n",
        "        # 16x16x512 -> 32x32x256\r\n",
        "        trans_conv2 = tf.layers.conv2d_transpose(inputs=trans_conv1_out,\r\n",
        "                                                 filters=256,\r\n",
        "                                                 kernel_size=[5,5],\r\n",
        "                                                 strides=[2,2],\r\n",
        "                                                 padding=\"SAME\",\r\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                                 name=\"trans_conv2\")\r\n",
        "        batch_trans_conv2 = tf.layers.batch_normalization(inputs = trans_conv2,\r\n",
        "                                                          training=training,\r\n",
        "                                                          epsilon=EPSILON,\r\n",
        "                                                          name=\"batch_trans_conv2\")\r\n",
        "        trans_conv2_out = tf.nn.leaky_relu(batch_trans_conv2,\r\n",
        "                                           name=\"trans_conv2_out\")\r\n",
        "        \r\n",
        "        # 32x32x256 -> 64x64x128\r\n",
        "        trans_conv3 = tf.layers.conv2d_transpose(inputs=trans_conv2_out,\r\n",
        "                                                 filters=128,\r\n",
        "                                                 kernel_size=[5,5],\r\n",
        "                                                 strides=[2,2],\r\n",
        "                                                 padding=\"SAME\",\r\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                                 name=\"trans_conv3\")\r\n",
        "        batch_trans_conv3 = tf.layers.batch_normalization(inputs = trans_conv3,\r\n",
        "                                                          training=training,\r\n",
        "                                                          epsilon=EPSILON,\r\n",
        "                                                          name=\"batch_trans_conv3\")\r\n",
        "        trans_conv3_out = tf.nn.leaky_relu(batch_trans_conv3,\r\n",
        "                                           name=\"trans_conv3_out\")\r\n",
        "        \r\n",
        "        # 64x64x128 -> 128x128x64\r\n",
        "        trans_conv4 = tf.layers.conv2d_transpose(inputs=trans_conv3_out,\r\n",
        "                                                 filters=64,\r\n",
        "                                                 kernel_size=[5,5],\r\n",
        "                                                 strides=[2,2],\r\n",
        "                                                 padding=\"SAME\",\r\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                                 name=\"trans_conv4\")\r\n",
        "        batch_trans_conv4 = tf.layers.batch_normalization(inputs = trans_conv4,\r\n",
        "                                                          training=training,\r\n",
        "                                                          epsilon=EPSILON,\r\n",
        "                                                          name=\"batch_trans_conv4\")\r\n",
        "        trans_conv4_out = tf.nn.leaky_relu(batch_trans_conv4,\r\n",
        "                                           name=\"trans_conv4_out\")\r\n",
        "        \r\n",
        "        # 128x128x64 -> 128x128x3\r\n",
        "        logits = tf.layers.conv2d_transpose(inputs=trans_conv4_out,\r\n",
        "                                            filters=3,\r\n",
        "                                            kernel_size=[5,5],\r\n",
        "                                            strides=[1,1],\r\n",
        "                                            padding=\"SAME\",\r\n",
        "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                            name=\"logits\")\r\n",
        "        out = tf.tanh(logits, name=\"out\")\r\n",
        "        return out"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLI0q_kD5UP0"
      },
      "source": [
        "def discriminator(x, reuse):\r\n",
        "    with tf.variable_scope(\"discriminator\", reuse=reuse): \r\n",
        "        \r\n",
        "        # 128*128*3 -> 64x64x64 \r\n",
        "        conv1 = tf.layers.conv2d(inputs=x,\r\n",
        "                                 filters=64,\r\n",
        "                                 kernel_size=[5,5],\r\n",
        "                                 strides=[2,2],\r\n",
        "                                 padding=\"SAME\",\r\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                 name='conv1')\r\n",
        "        batch_norm1 = tf.layers.batch_normalization(conv1,\r\n",
        "                                                    training=True,\r\n",
        "                                                    epsilon=EPSILON,\r\n",
        "                                                    name='batch_norm1')\r\n",
        "        conv1_out = tf.nn.leaky_relu(batch_norm1,\r\n",
        "                                     name=\"conv1_out\")\r\n",
        "        \r\n",
        "        # 64x64x64-> 32x32x128 \r\n",
        "        conv2 = tf.layers.conv2d(inputs=conv1_out,\r\n",
        "                                 filters=128,\r\n",
        "                                 kernel_size=[5, 5],\r\n",
        "                                 strides=[2, 2],\r\n",
        "                                 padding=\"SAME\",\r\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                 name='conv2')\r\n",
        "        batch_norm2 = tf.layers.batch_normalization(conv2,\r\n",
        "                                                    training=True,\r\n",
        "                                                    epsilon=EPSILON,\r\n",
        "                                                    name='batch_norm2')\r\n",
        "        conv2_out = tf.nn.leaky_relu(batch_norm2,\r\n",
        "                                     name=\"conv2_out\")\r\n",
        "        \r\n",
        "        # 32x32x128 -> 16x16x256  \r\n",
        "        conv3 = tf.layers.conv2d(inputs=conv2_out,\r\n",
        "                                 filters=256,\r\n",
        "                                 kernel_size=[5, 5],\r\n",
        "                                 strides=[2, 2],\r\n",
        "                                 padding=\"SAME\",\r\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                 name='conv3')\r\n",
        "        batch_norm3 = tf.layers.batch_normalization(conv3,\r\n",
        "                                                    training=True,\r\n",
        "                                                    epsilon=EPSILON,\r\n",
        "                                                    name='batch_norm3')\r\n",
        "        conv3_out = tf.nn.leaky_relu(batch_norm3,\r\n",
        "                                     name=\"conv3_out\")\r\n",
        "        \r\n",
        "        # 16x16x256 -> 16x16x512\r\n",
        "        conv4 = tf.layers.conv2d(inputs=conv3_out,\r\n",
        "                                 filters=512,\r\n",
        "                                 kernel_size=[5, 5],\r\n",
        "                                 strides=[1, 1],\r\n",
        "                                 padding=\"SAME\",\r\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                 name='conv4')\r\n",
        "        batch_norm4 = tf.layers.batch_normalization(conv4,\r\n",
        "                                                    training=True,\r\n",
        "                                                    epsilon=EPSILON,\r\n",
        "                                                    name='batch_norm4')\r\n",
        "        conv4_out = tf.nn.leaky_relu(batch_norm4,\r\n",
        "                                     name=\"conv4_out\")\r\n",
        "        \r\n",
        "        # 16x16x512 -> 8x8x1024\r\n",
        "        conv5 = tf.layers.conv2d(inputs=conv4_out,\r\n",
        "                                filters=1024,\r\n",
        "                                kernel_size=[5, 5],\r\n",
        "                                strides=[2, 2],\r\n",
        "                                padding=\"SAME\",\r\n",
        "                                kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\r\n",
        "                                name='conv5')\r\n",
        "        batch_norm5 = tf.layers.batch_normalization(conv5,\r\n",
        "                                                    training=True,\r\n",
        "                                                    epsilon=EPSILON,\r\n",
        "                                                    name='batch_norm5')\r\n",
        "        conv5_out = tf.nn.leaky_relu(batch_norm5,\r\n",
        "                                     name=\"conv5_out\")\r\n",
        "\r\n",
        "        flatten = tf.reshape(conv5_out, (-1, 8*8*1024))\r\n",
        "        logits = tf.layers.dense(inputs=flatten,\r\n",
        "                                 units=1,\r\n",
        "                                 activation=None)\r\n",
        "        out = tf.sigmoid(logits)\r\n",
        "        return out, logits"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e3zv1ls5XRW"
      },
      "source": [
        "def model_loss(input_real, input_z, output_channel_dim):\r\n",
        "    g_model = generator(input_z, output_channel_dim, True)\r\n",
        "\r\n",
        "    noisy_input_real = input_real + tf.random_normal(shape=tf.shape(input_real),\r\n",
        "                                                     mean=0.0,\r\n",
        "                                                     stddev=random.uniform(0.0, 0.1),\r\n",
        "                                                     dtype=tf.float32)\r\n",
        "    \r\n",
        "    d_model_real, d_logits_real = discriminator(noisy_input_real, reuse=False)\r\n",
        "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\r\n",
        "    \r\n",
        "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\r\n",
        "                                                                         labels=tf.ones_like(d_model_real)*random.uniform(0.9, 1.0)))\r\n",
        "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\r\n",
        "                                                                         labels=tf.zeros_like(d_model_fake)))\r\n",
        "    d_loss = tf.reduce_mean(0.5 * (d_loss_real + d_loss_fake))\r\n",
        "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\r\n",
        "                                                                    labels=tf.ones_like(d_model_fake)))\r\n",
        "    return d_loss, g_loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QapvJ8ey5cPL"
      },
      "source": [
        "def model_optimizers(d_loss, g_loss):\r\n",
        "    t_vars = tf.trainable_variables()\r\n",
        "    g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\r\n",
        "    d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\r\n",
        "    \r\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n",
        "    gen_updates = [op for op in update_ops if op.name.startswith('generator') or op.name.startswith('discriminator')]\r\n",
        "    \r\n",
        "    with tf.control_dependencies(gen_updates):\r\n",
        "        d_train_opt = tf.train.AdamOptimizer(learning_rate=LR_D, beta1=BETA1).minimize(d_loss, var_list=d_vars)\r\n",
        "        g_train_opt = tf.train.AdamOptimizer(learning_rate=LR_G, beta1=BETA1).minimize(g_loss, var_list=g_vars)  \r\n",
        "    return d_train_opt, g_train_opt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfOeDdvg5eJP"
      },
      "source": [
        "\r\n",
        "def model_inputs(real_dim, z_dim):\r\n",
        "    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='inputs_real')\r\n",
        "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\r\n",
        "    learning_rate_G = tf.placeholder(tf.float32, name=\"lr_g\")\r\n",
        "    learning_rate_D = tf.placeholder(tf.float32, name=\"lr_d\")\r\n",
        "    return inputs_real, inputs_z, learning_rate_G, learning_rate_D"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUNMJAT_5gK5"
      },
      "source": [
        "\r\n",
        "def show_samples(sample_images, name, epoch):\r\n",
        "    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE, IMAGE_SIZE))\r\n",
        "    for index, axis in enumerate(axes):\r\n",
        "        axis.axis('off')\r\n",
        "        image_array = sample_images[index]\r\n",
        "        axis.imshow(image_array)\r\n",
        "        image = Image.fromarray(image_array)\r\n",
        "        image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\") \r\n",
        "    plt.savefig(name+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\r\n",
        "    plt.show()\r\n",
        "    plt.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EhecLdJ5iWx"
      },
      "source": [
        "\r\n",
        "def test(sess, input_z, out_channel_dim, epoch):\r\n",
        "    example_z = np.random.uniform(-1, 1, size=[SAMPLES_TO_SHOW, input_z.get_shape().as_list()[-1]])\r\n",
        "    samples = sess.run(generator(input_z, out_channel_dim, False), feed_dict={input_z: example_z})\r\n",
        "    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\r\n",
        "    show_samples(sample_images, OUTPUT_DIR + \"samples\", epoch)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIV63wZ-5kQa"
      },
      "source": [
        "def summarize_epoch(epoch, sess, d_losses, g_losses, input_z, data_shape, saver):\r\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch, EPOCHS),\r\n",
        "          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-MINIBATCH_SIZE:])),\r\n",
        "          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-MINIBATCH_SIZE:])))\r\n",
        "    fig, ax = plt.subplots()\r\n",
        "    plt.plot(d_losses, label='Discriminator', alpha=0.6)\r\n",
        "    plt.plot(g_losses, label='Generator', alpha=0.6)\r\n",
        "    plt.title(\"Losses\")\r\n",
        "    plt.legend()\r\n",
        "    plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\r\n",
        "    plt.show()\r\n",
        "    plt.close()\r\n",
        "    saver.save(sess, OUTPUT_DIR + \"model_\" + str(epoch) + \".ckpt\")\r\n",
        "    test(sess, input_z, data_shape[3], epoch)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu-DYbvU5l9R"
      },
      "source": [
        "def get_batch(dataset):\r\n",
        "    files = random.sample(dataset, BATCH_SIZE)\r\n",
        "    batch = []\r\n",
        "    for file in files:\r\n",
        "        if random.choice([True, False]):\r\n",
        "            batch.append(np.asarray(Image.open(file).transpose(Image.FLIP_LEFT_RIGHT)))\r\n",
        "        else:\r\n",
        "            batch.append(np.asarray(Image.open(file)))                     \r\n",
        "    batch = np.asarray(batch)\r\n",
        "    normalized_batch = (batch / 127.5) - 1.0\r\n",
        "    return normalized_batch, files"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttQxZG9-5nsx"
      },
      "source": [
        "def train(data_shape, epoch, checkpoint_path):\r\n",
        "    input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], NOISE_SIZE)\r\n",
        "    d_loss, g_loss = model_loss(input_images, input_z, data_shape[3])\r\n",
        "    d_opt, g_opt = model_optimizers(d_loss, g_loss)\r\n",
        "    \r\n",
        "    with tf.Session() as sess:\r\n",
        "        sess.run(tf.global_variables_initializer())\r\n",
        "        saver = tf.train.Saver()\r\n",
        "        if checkpoint_path is not None:\r\n",
        "            saver.restore(sess, checkpoint_path)\r\n",
        "            \r\n",
        "        iteration = 0\r\n",
        "        d_losses = []\r\n",
        "        g_losses = []\r\n",
        "        \r\n",
        "        for epoch in range(EPOCH, EPOCHS):        \r\n",
        "            epoch += 1\r\n",
        "            epoch_dataset = DATASET.copy()\r\n",
        "            \r\n",
        "            for i in range(MINIBATCH_SIZE):\r\n",
        "                iteration_start_time = time.time()\r\n",
        "                iteration += 1\r\n",
        "                batch_images, used_files = get_batch(epoch_dataset)\r\n",
        "                [epoch_dataset.remove(file) for file in used_files]\r\n",
        "                \r\n",
        "                batch_z = np.random.uniform(-1, 1, size=(BATCH_SIZE, NOISE_SIZE))\r\n",
        "                _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: LR_D})\r\n",
        "                _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: LR_G})\r\n",
        "                d_losses.append(d_loss.eval({input_z: batch_z, input_images: batch_images}))\r\n",
        "                g_losses.append(g_loss.eval({input_z: batch_z}))\r\n",
        "                \r\n",
        "                elapsed_time = round(time.time()-iteration_start_time, 3)\r\n",
        "                remaining_files = len(epoch_dataset)\r\n",
        "                print(\"\\rEpoch: \" + str(epoch) +\r\n",
        "                      \", iteration: \" + str(iteration) + \r\n",
        "                      \", d_loss: \" + str(round(d_losses[-1], 3)) +\r\n",
        "                      \", g_loss: \" + str(round(g_losses[-1], 3)) +\r\n",
        "                      \", duration: \" + str(elapsed_time) + \r\n",
        "                      \", minutes remaining: \" + str(round(remaining_files/BATCH_SIZE*elapsed_time/60, 1)) +\r\n",
        "                      \", remaining files in batch: \" + str(remaining_files)\r\n",
        "                      , sep=' ', end=' ', flush=True)\r\n",
        "                \r\n",
        "            summarize_epoch(epoch, sess, d_losses, g_losses, input_z, data_shape, saver)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLzHQ6jX5pfM"
      },
      "source": [
        "# Hyperparameters\r\n",
        "IMAGE_SIZE = 128\r\n",
        "NOISE_SIZE = 100\r\n",
        "LR_D = 0.00004\r\n",
        "LR_G = 0.0002\r\n",
        "BATCH_SIZE = 64\r\n",
        "EPOCH = 0 # Non-zero only if we are resuming training with model checkpoint\r\n",
        "EPOCHS = 5 #EPOCH + number of epochs to perform\r\n",
        "BETA1 = 0.5\r\n",
        "WEIGHT_INIT_STDDEV = 0.02\r\n",
        "EPSILON = 0.00005\r\n",
        "SAMPLES_TO_SHOW = 5"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "Nr6RwZP35q7c",
        "outputId": "c89dce8a-1e5f-4c31-cd08-4c4ddfc465d4"
      },
      "source": [
        "# Data (https://www.kaggle.com/greg115/celebrities-100k)\r\n",
        "BASE_PATH = \"../input/\"\r\n",
        "DATASET_LIST_PATH = BASE_PATH + \"100k.txt\"\r\n",
        "INPUT_DATA_DIR = BASE_PATH + \"100k/100k/\"\r\n",
        "OUTPUT_DIR = \"./\"\r\n",
        "DATASET = [INPUT_DATA_DIR + str(line).rstrip() for line in open(DATASET_LIST_PATH,\"r\")]\r\n",
        "DATASET_SIZE = len(DATASET) \r\n",
        "MINIBATCH_SIZE = DATASET_SIZE // BATCH_SIZE\r\n",
        "\r\n",
        "# Optional - model path to resume training\r\n",
        "#MODEL_PATH = BASE_PATH + \"models/\" + \"model_\" + str(EPOCH) + \".ckpt\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8101dac4feac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mINPUT_DATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBASE_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"100k/100k/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mDATASET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mINPUT_DATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_LIST_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mDATASET_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mMINIBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATASET_SIZE\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/100k.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "EytGqvz15tFr",
        "outputId": "42aea547-725a-4006-ef16-8c1e3f46c176"
      },
      "source": [
        "# Training\r\n",
        "with tf.Graph().as_default():\r\n",
        "    train(data_shape=(DATASET_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3),\r\n",
        "          epoch=EPOCH,\r\n",
        "          checkpoint_path=None)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-21bfaf01d033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     train(data_shape=(DATASET_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3),\n\u001b[0m\u001b[1;32m      4\u001b[0m           \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           checkpoint_path=None)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DATASET_SIZE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qCKiZRm5vjZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}