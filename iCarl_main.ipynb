{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "iCarl_main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinsapre/GANfaces_iCarl/blob/main/iCarl_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y73F_4X-AuAy"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import keras.layers as L\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.applications import DenseNet121\n",
        "from keras.applications import VGG16\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from os import listdir\n",
        "import cv2\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VttqVxphAuA6"
      },
      "source": [
        " For multi-task learning, branching NN out\n",
        "- one branch predicts the class of FashionMNIST object (classification)\n",
        "- other branch predicts whether the object is a \"top\" or not (0/1 output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAzJyR_jyokb"
      },
      "source": [
        "def calc_feature_vectors(model, images):\r\n",
        "    #extract feature vector from a layer of ALREADY-TRAINED model\r\n",
        "    #size of feature vector=784\r\n",
        "    feature_model=keras.Model(inputs=model.inputs, outputs=model.get_layer(\"flatten\").output)\r\n",
        "    feature_vectors=feature_model.predict(images)\r\n",
        "    norms=np.linalg.norm(feature_vectors, keepdims=True, axis=1)\r\n",
        "    feature_vectors=feature_vectors/norms\r\n",
        "    return feature_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS8E3gY2ytYc"
      },
      "source": [
        "def create_exemplar_set(mem_size, n_classes, feature_vectors, labels, reconstruct=False):\r\n",
        "    per_class=mem_size//n_classes\r\n",
        "\r\n",
        "    class_vectors={}\r\n",
        "    mean_class_vectors={}\r\n",
        "    class_vectors_distances={}\r\n",
        "\r\n",
        "    #init dicts\r\n",
        "    for i in range(n_classes):\r\n",
        "        class_vectors[i]=[]\r\n",
        "        mean_class_vectors[i]=[]\r\n",
        "        class_vectors_distances[i]=[]\r\n",
        "    \r\n",
        "    #vectors belonging to class i go in the list in key i of class_vectors\r\n",
        "    for i in range(len(labels)):\r\n",
        "        class_vectors[labels[i]].append(feature_vectors[i])\r\n",
        "\r\n",
        "    #calculate mean class vectors by summing all vectors per class and diving by length\r\n",
        "    for i in range(n_classes):\r\n",
        "        mean_class_vectors[i]=np.sum(class_vectors[i], axis=0)/len(class_vectors[i])\r\n",
        "    \r\n",
        "    #calculating distances from mean\r\n",
        "    for i in range(len(labels)):\r\n",
        "        class_vectors_distances[labels[i]].append(np.linalg.norm(mean_class_vectors[labels[i]] - feature_vectors[i]))\r\n",
        "\r\n",
        "    #sorting vectors by their corresponding distances from class means\r\n",
        "    for i in range(n_classes):\r\n",
        "        class_vectors_distances[i], class_vectors[i] = (list(t) for t in zip(*sorted(zip(class_vectors_distances[i], class_vectors[i]))))\r\n",
        "\r\n",
        "    exemplars_x = []\r\n",
        "    exemplars_y = []\r\n",
        "\r\n",
        "    #choose 'per_class' number of vectors\r\n",
        "    for i in range(n_classes):\r\n",
        "        exemplars_x.append(class_vectors[i][:per_class])\r\n",
        "        exemplars_y+= per_class*[i]\r\n",
        "\r\n",
        "    return exemplars_x, exemplars_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k514jxJ6ywEn"
      },
      "source": [
        "def classify(X, mean_class_vectors, n_classes):\r\n",
        "    #image embedding created\r\n",
        "    image_vector=calc_feature_vectors(model, X)\r\n",
        "    #distance to all mean vectors calculated\r\n",
        "    distances=[]\r\n",
        "    indices=[x for x in range(n_classes)]\r\n",
        "    #sort class values by their corresponding distances\r\n",
        "    for i in range(n_classes):\r\n",
        "        distances.append(np.linalg.norm(mean_class_vectors[i] - image_vector))\r\n",
        "    distances, indices = (list(t) for t in zip(*sorted(zip(distances, indices))))\r\n",
        "    #return the class with the least distance\r\n",
        "    return indices[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8-eX21QAuA6"
      },
      "source": [
        "def create_model(n_classes, input_dim, cl_weight, b_weight, lr):\n",
        "    '''\n",
        "        Creating categorical classification model\n",
        "    '''\n",
        "    \n",
        "    inputs=L.Input((input_dim,input_dim,1), name='input_layer_common')\n",
        "    \n",
        "    xc=L.Conv2D(64, kernel_size=3, padding='same', strides=1, name='conv1_c')(inputs)\n",
        "    xc=L.LeakyReLU(0.2, name='relu1_c')(xc)\n",
        "    xc=L.Conv2D(64, kernel_size=3, padding='same', strides=1, name='conv2_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu2_c')(xc)\n",
        "    xc=L.MaxPool2D(pool_size=2, strides=2, name='pool1_c')(xc)\n",
        "    \n",
        "    xc=L.Conv2D(32, kernel_size=3, padding='same', strides=1, name='conv3_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu3_c')(xc)\n",
        "    xc=L.Conv2D(32, kernel_size=3, padding='same', strides=1, name='conv4_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu4_c')(xc)\n",
        "    xc=L.MaxPool2D(pool_size=2, strides=2, name='pool2_c')(xc)\n",
        "    \n",
        "    xc=L.Conv2D(16, kernel_size=3, padding='same', strides=1, name='conv5_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu5_c')(xc)\n",
        "    xc=L.Conv2D(16, kernel_size=3, padding='same', strides=1, name='conv6_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu6_c')(xc)\n",
        "    xc=L.MaxPool2D(pool_size=2, strides=2, name='pool3_c')(xc)\n",
        "    \n",
        "    xc=L.Flatten(name='flatten_c')(xc)\n",
        "    outputc=L.Dense(n_classes, activation='softmax', name='outputc')(xc)\n",
        "    #outputc=L.Softmax(name='outputc')(xc)\n",
        "    \n",
        "    '''\n",
        "        Creating binary classification model (a top/not a top)\n",
        "    '''\n",
        "    x=L.Conv2D(32, kernel_size=3, padding='same', strides=1, name='conv1_b')(inputs)\n",
        "    x=L.LeakyReLU(0.2, name='relu1_b')(x)\n",
        "    x=L.Conv2D(32, kernel_size=3, padding='same', strides=1, name='conv2_b')(x)\n",
        "    x=L.LeakyReLU(0.2, name='relu2_b')(x)\n",
        "    x=L.MaxPool2D(pool_size=2, strides=2, name='pool1_b')(x)\n",
        "    \n",
        "    x=L.Conv2D(16, kernel_size=3, padding='same', strides=1, name='conv3_b')(x)\n",
        "    x=L.LeakyReLU(0.2, name='relu3_b')(x)\n",
        "    x=L.Conv2D(16, kernel_size=3, padding='same', strides=1, name='conv4_b')(x)\n",
        "    x=L.LeakyReLU(0.2, name='relu4_b')(x)\n",
        "    x=L.MaxPool2D(pool_size=2, strides=2, name='pool2_b')(x)\n",
        "    \n",
        "    x=L.Flatten(name='flatten_b')(x)\n",
        "    outputb=L.Dense(1, activation='sigmoid', name='outputb')(x)\n",
        "    #outputb=L.Activation('sigmoid', name=\"outputb\")(x)\n",
        "\n",
        "    '''\n",
        "        Combining both models\n",
        "    '''\n",
        "    model=keras.Model(inputs=inputs, outputs=[outputc, outputb])\n",
        "    losses={'outputc':'categorical_crossentropy', 'outputb':'binary_crossentropy'}\n",
        "    loss_weights=[cl_weight, b_weight]\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr), \n",
        "                      loss=losses,\n",
        "                      loss_weights=loss_weights,\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY707Bfw3YB-"
      },
      "source": [
        "def fine_tune(model, n_classes, cl_weight, b_weight, lr):\r\n",
        "    penultimate_layer=model.get_layer('flatten_c').output\r\n",
        "    outputc=L.Dense(n_classes, activation='softmax', name='outputc')(penultimate_layer)\r\n",
        "\r\n",
        "    outputb=model.get_layer('outputb').output\r\n",
        "    new_model=keras.Model(inputs=model.inputs,\r\n",
        "                        outputs=[outputc, outputb])\r\n",
        "    for layer in new_model.layers:\r\n",
        "        if(layer.name not in ['conv4_b', 'outputb', 'conv6_c', 'outputc']):\r\n",
        "            layer.trainable=False\r\n",
        "    losses={'outputc':'categorical_crossentropy', 'outputb':'binary_crossentropy'}\r\n",
        "    loss_weights=[cl_weight, b_weight]\r\n",
        "    new_model.compile(optimizer=keras.optimizers.Adam(lr), loss=losses, loss_weights=loss_weights, metrics=['accuracy'])\r\n",
        "\r\n",
        "    return new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3b09_gxAuA7"
      },
      "source": [
        "def train_model(model, n_epochs:int, x_train, y_trainc, y_trainb, validation_split):\n",
        "    history=model.fit(x=x_train, \n",
        "                      y={'outputc':y_trainc, 'outputb': y_trainb}, \n",
        "                      epochs=n_epochs, \n",
        "                      validation_split=0.1,\n",
        "                      shuffle=True,\n",
        "                      verbose=verbose)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-NFMEylIi8F"
      },
      "source": [
        "def synthesize_dataset(x_train, y_train, n):\r\n",
        "    labeldict={}\r\n",
        "    labeldict[0]=1\r\n",
        "    labeldict[1]=0\r\n",
        "    labeldict[2]=1\r\n",
        "    labeldict[3]=1\r\n",
        "    labeldict[4]=1\r\n",
        "    labeldict[5]=0\r\n",
        "    labeldict[6]=1\r\n",
        "    labeldict[7]=0\r\n",
        "    labeldict[8]=0\r\n",
        "    labeldict[9]=0\r\n",
        "\r\n",
        "    classes=[x for x in range(n)]\r\n",
        "    selected_indices=[]\r\n",
        "    unselected_indices=[]\r\n",
        "    for i in range(len(y_train)):\r\n",
        "        if(y_train[i] in classes):\r\n",
        "            selected_indices.append(i)\r\n",
        "        else:\r\n",
        "            unselected_indices.append(i)\r\n",
        "\r\n",
        "    x_train_new=np.delete(x_train, [unselected_indices], axis=0)\r\n",
        "    y_train=[y_train[i] for i in selected_indices]\r\n",
        "\r\n",
        "    y_trainb=[]\r\n",
        "    y_testb=[]\r\n",
        "    y_trainc=keras.utils.to_categorical(y_train)\r\n",
        "    #y_testc=keras.utils.to_categorical(y_test)\r\n",
        "    \r\n",
        "    for i in range(len(y_train)):\r\n",
        "        y_trainb.append(labeldict[y_train[i]])\r\n",
        "        \r\n",
        "    #for i in range(len(y_test)):\r\n",
        "    #    y_testb.append(labeldict[y_test[i]])\r\n",
        "    \r\n",
        "    return x_train_new, np.asarray(y_trainc), np.asarray(y_trainb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtyWF1__3lhl"
      },
      "source": [
        "lr=0.0005\r\n",
        "epochs=5\r\n",
        "init_classes=3\r\n",
        "verbose=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpOoDTFlgG2D",
        "outputId": "ccf10289-b0eb-4dac-a9bd-f228dd20d368",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAea44pFp7Bz",
        "outputId": "12ffaea2-47a3-4c2e-f936-e8970bb936b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "root='/content/drive/MyDrive/stylegan/'\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "        rescale=1./255,\r\n",
        "        shear_range=0.2,\r\n",
        "        zoom_range=0.2,\r\n",
        "        horizontal_flip=True)\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "        root,\r\n",
        "        target_size=(224, 224),\r\n",
        "        batch_size=32,\r\n",
        "        class_mode='binary')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwrcNzQZAuA8"
      },
      "source": [
        "'''\n",
        "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train=x_train.reshape((x_train.shape[0], 28,28,1))\n",
        "x_test=x_test.reshape((x_test.shape[0],28,28,1))\n",
        "x_train=x_train/255.0\n",
        "x_test=x_test/255.0\n",
        "'''\n",
        "#first iteration of training with 5 classes\n",
        "xt, ytc, ytb=synthesize_dataset(x_train, y_train, init_classes)\n",
        "model=create_model(n_classes=init_classes, input_dim=28, cl_weight=1.0, b_weight=1.5, lr=lr)\n",
        "train_model(model, epochs, xt, ytc, ytb, validation_split=0.1)\n",
        "#second iteration of training with 9 classes\n",
        "xt, ytc, ytb=synthesize_dataset(x_train, y_train, init_classes+7)\n",
        "ftmodel=fine_tune(model, n_classes=init_classes+7, cl_weight=1, b_weight=1, lr=0.001)\n",
        "train_model(ftmodel, epochs, xt, ytc, ytb, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}