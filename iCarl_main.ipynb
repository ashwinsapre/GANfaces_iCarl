{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "iCarl_main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinsapre/GANfaces_iCarl/blob/main/iCarl_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y73F_4X-AuAy"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import keras.layers as L\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.applications import DenseNet121\n",
        "from keras.applications import VGG16\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3d2b1v4Nik8",
        "outputId": "cc10be5c-a78d-4fae-c25c-fa579eda737c"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VttqVxphAuA6"
      },
      "source": [
        " For multi-task learning, branching NN out\n",
        "- one branch predicts the class of FashionMNIST object (classification)\n",
        "- other branch predicts whether the object is a \"top\" or not (0/1 output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8-eX21QAuA6"
      },
      "source": [
        "def create_model(n_classes, input_dim, cl_weight, b_weight, lr):\n",
        "    '''\n",
        "        Creating categorical classification model\n",
        "    '''\n",
        "    \n",
        "    inputs=L.Input((input_dim,input_dim,1), name='input_layer_common')\n",
        "    \n",
        "    xc=L.Conv2D(64, kernel_size=3, padding='same', strides=1, name='conv1_c')(inputs)\n",
        "    xc=L.LeakyReLU(0.2, name='relu1_c')(xc)\n",
        "    xc=L.Conv2D(64, kernel_size=3, padding='same', strides=1, name='conv2_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu2_c')(xc)\n",
        "    xc=L.MaxPool2D(pool_size=2, strides=2, name='pool1_c')(xc)\n",
        "    \n",
        "    xc=L.Conv2D(32, kernel_size=3, padding='same', strides=1, name='conv3_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu3_c')(xc)\n",
        "    xc=L.Conv2D(32, kernel_size=3, padding='same', strides=1, name='conv4_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu4_c')(xc)\n",
        "    xc=L.MaxPool2D(pool_size=2, strides=2, name='pool2_c')(xc)\n",
        "    \n",
        "    xc=L.Conv2D(16, kernel_size=3, padding='same', strides=1, name='conv5_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu5_c')(xc)\n",
        "    xc=L.Conv2D(16, kernel_size=3, padding='same', strides=1, name='conv6_c')(xc)\n",
        "    xc=L.LeakyReLU(0.2, name='relu6_c')(xc)\n",
        "    xc=L.MaxPool2D(pool_size=2, strides=2, name='pool3_c')(xc)\n",
        "    \n",
        "    xc=L.Flatten(name='flatten_c')(xc)\n",
        "    outputc=L.Dense(n_classes, activation='softmax', name='outputc')(xc)\n",
        "    #outputc=L.Softmax(name='outputc')(xc)\n",
        "    \n",
        "    '''\n",
        "        Creating binary classification model (a top/not a top)\n",
        "    '''\n",
        "    x=L.Conv2D(32, kernel_size=3, padding='same', strides=1, name='conv1_b')(inputs)\n",
        "    x=L.LeakyReLU(0.2, name='relu1_b')(x)\n",
        "    x=L.Conv2D(32, kernel_size=3, padding='same', strides=1, name='conv2_b')(x)\n",
        "    x=L.LeakyReLU(0.2, name='relu2_b')(x)\n",
        "    x=L.MaxPool2D(pool_size=2, strides=2, name='pool1_b')(x)\n",
        "    \n",
        "    x=L.Conv2D(16, kernel_size=3, padding='same', strides=1, name='conv3_b')(x)\n",
        "    x=L.LeakyReLU(0.2, name='relu3_b')(x)\n",
        "    x=L.Conv2D(16, kernel_size=3, padding='same', strides=1, name='conv4_b')(x)\n",
        "    x=L.LeakyReLU(0.2, name='relu4_b')(x)\n",
        "    x=L.MaxPool2D(pool_size=2, strides=2, name='pool2_b')(x)\n",
        "    \n",
        "    x=L.Flatten(name='flatten_b')(x)\n",
        "    outputb=L.Dense(1, activation='sigmoid', name='outputb')(x)\n",
        "    #outputb=L.Activation('sigmoid', name=\"outputb\")(x)\n",
        "\n",
        "    '''\n",
        "        Combining both models\n",
        "    '''\n",
        "    model=keras.Model(inputs=inputs, outputs=[outputc, outputb])\n",
        "    losses={'outputc':'categorical_crossentropy', 'outputb':'binary_crossentropy'}\n",
        "    loss_weights=[cl_weight, b_weight]\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr), \n",
        "                      loss=losses,\n",
        "                      loss_weights=loss_weights,\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY707Bfw3YB-"
      },
      "source": [
        "def fine_tune(model, n_classes, cl_weight, b_weight, lr):\r\n",
        "    penultimate_layer=model.get_layer('flatten_c').output\r\n",
        "    penultimate_layer=L.Dense(n_classes)(penultimate_layer)\r\n",
        "    new_outputc=L.Softmax(name='new_outputc')(penultimate_layer)\r\n",
        "\r\n",
        "    outputb=model.get_layer('outputb').output\r\n",
        "    new_model=keras.Model(inputs=model.inputs,\r\n",
        "                        outputs=[new_outputc, outputb])\r\n",
        "    for layer in new_model.layers:\r\n",
        "        if(layer.name not in ['conv4_b', 'outputb', 'conv6_c', 'outputc']):\r\n",
        "            layer.trainable=False\r\n",
        "    losses={'outputc':'categorical_crossentropy', 'outputb':'binary_crossentropy'}\r\n",
        "    loss_weights=[cl_weight, b_weight]\r\n",
        "    new_model.compile(optimizer=keras.optimizers.Adam(lr), loss=losses, loss_weights=loss_weights, metrics=['accuracy'])\r\n",
        "\r\n",
        "    return new_model"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3b09_gxAuA7"
      },
      "source": [
        "def train_model(model, n_epochs:int, x_train, y_trainc, y_trainb, validation_split):\n",
        "    history=model.fit(x=x_train, \n",
        "                      y={'outputc':y_trainc, 'outputb': y_trainb}, \n",
        "                      epochs=n_epochs, \n",
        "                      validation_split=0.1,\n",
        "                      shuffle=True,\n",
        "                      verbose=2)\n",
        "    return model"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd5OfDiIAuA7"
      },
      "source": [
        "def calc_feature_vectors(model, images):\n",
        "    #extract feature vector from a layer of ALREADY-TRAINED model\n",
        "    #size of feature vector=784\n",
        "    feature_model=keras.Model(inputs=model.inputs, outputs=model.get_layer(\"flatten\").output)\n",
        "    feature_vectors=feature_model.predict(images)\n",
        "    norms=np.linalg.norm(feature_vectors, keepdims=True, axis=1)\n",
        "    feature_vectors=feature_vectors/norms\n",
        "    return feature_vectors"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jymSn4mRtO5H"
      },
      "source": [
        "def create_exemplar_set(mem_size, n_classes, feature_vectors, labels, reconstruct=False):\r\n",
        "    per_class=mem_size//n_classes\r\n",
        "\r\n",
        "    class_vectors={}\r\n",
        "    mean_class_vectors={}\r\n",
        "    class_vectors_distances={}\r\n",
        "\r\n",
        "    #init dicts\r\n",
        "    for i in range(n_classes):\r\n",
        "      class_vectors[i]=[]\r\n",
        "      mean_class_vectors[i]=[]\r\n",
        "      class_vectors_distances[i]=[]\r\n",
        "    \r\n",
        "    #vectors belonging to class i go in the list in key i of class_vectors\r\n",
        "    for i in range(len(labels)):\r\n",
        "      class_vectors[labels[i]].append(feature_vectors[i])\r\n",
        "\r\n",
        "    #calculate mean class vectors by summing all vectors per class and diving by length\r\n",
        "    for i in range(n_classes):\r\n",
        "      mean_class_vectors[i]=np.sum(class_vectors[i], axis=0)/len(class_vectors[i])\r\n",
        "    \r\n",
        "    #calculating distances from mean\r\n",
        "    for i in range(len(labels)):\r\n",
        "      class_vectors_distances[labels[i]].append(np.linalg.norm(mean_class_vectors[labels[i]] - feature_vectors[i]))\r\n",
        "\r\n",
        "    #sorting vectors by their corresponding distances from class means\r\n",
        "    for i in range(n_classes):\r\n",
        "      class_vectors_distances[i], class_vectors[i] = (list(t) for t in zip(*sorted(zip(class_vectors_distances[i], class_vectors[i]))))\r\n",
        "\r\n",
        "    exemplars_x = []\r\n",
        "    exemplars_y = []\r\n",
        "\r\n",
        "    #choose 'per_class' number of vectors\r\n",
        "    for i in range(n_classes):\r\n",
        "      exemplars_x.append(class_vectors[i][:per_class])\r\n",
        "      exemplars_y+= per_class*[i]\r\n",
        "\r\n",
        "    return exemplars_x, exemplars_y"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Wu1QlhAuA7"
      },
      "source": [
        "def classify(X, mean_class_vectors, n_classes):\n",
        "    #image embedding created\n",
        "    image_vector=calc_feature_vectors(model, X)\n",
        "    #distance to all mean vectors calculated\n",
        "    distances=[]\n",
        "    indices=[x for x in range(n_classes)]\n",
        "    #sort class values by their corresponding distances\n",
        "    for i in range(n_classes):\n",
        "      distances.append(np.linalg.norm(mean_class_vectors[i] - image_vector))\n",
        "    distances, indices = (list(t) for t in zip(*sorted(zip(distances, indices))))\n",
        "    #return the class with the least distance\n",
        "    return indices[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l48ckkJAuA8"
      },
      "source": [
        "def transform_outputs(y_train, y_test):\n",
        "    labeldict={}\n",
        "    labeldict[0]=1\n",
        "    labeldict[1]=0\n",
        "    labeldict[2]=1\n",
        "    labeldict[3]=1\n",
        "    labeldict[4]=1\n",
        "    labeldict[5]=0\n",
        "    labeldict[6]=1\n",
        "    labeldict[7]=0\n",
        "    labeldict[8]=0\n",
        "    labeldict[9]=0\n",
        "    \n",
        "    y_trainb=[]\n",
        "    y_testb=[]\n",
        "    y_trainc=keras.utils.to_categorical(y_train)\n",
        "    y_testc=keras.utils.to_categorical(y_test)\n",
        "    \n",
        "    for i in range(len(y_train)):\n",
        "        y_trainb.append(labeldict[y_train[i]])\n",
        "        \n",
        "    for i in range(len(y_test)):\n",
        "        y_testb.append(labeldict[y_test[i]])\n",
        "    \n",
        "    return y_trainc, y_testc, np.asarray(y_trainb), np.asarray(y_testb)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwrcNzQZAuA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c9c4d5-00b5-49d1-c0b7-2a13bb9bcf75"
      },
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train=x_train.reshape((x_train.shape[0], 28,28,1))\n",
        "x_test=x_test.reshape((x_test.shape[0],28,28,1))\n",
        "y_trainc, y_testc, y_trainb, y_testb=transform_outputs(y_train, y_test)\n",
        "x_train=x_train/255.0\n",
        "x_test=x_test/255.0\n",
        "\n",
        "#np.savetxt(\"ytrain_transformed.csv\", y_train, delimiter=',')\n",
        "#np.savetxt(\"ytest_transformed.csv\", y_test, delimiter=',')\n",
        "    \n",
        "#y_train=np.loadtxt(open(\"ytrain_transformed.csv\", \"rb\"), delimiter=',')\n",
        "#y_test=np.loadtxt(open(\"ytest_transformed.csv\", \"rb\"), delimiter=',')\n",
        "    \n",
        "model=create_model(n_classes=10, input_dim=28, cl_weight=1.0, b_weight=1.5, lr=0.0005)\n",
        "    \n",
        "train_model(model, 10, x_train, y_trainc, y_trainb, validation_split=0.1)\n",
        "model.save(\"trainedmodel.h5\")\n",
        "#-----------------------------------------------------------------------#"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 - 12s - loss: 0.6772 - outputc_loss: 0.5668 - outputb_loss: 0.0736 - outputc_accuracy: 0.7949 - outputb_accuracy: 0.9752 - val_loss: 0.4685 - val_outputc_loss: 0.4053 - val_outputb_loss: 0.0421 - val_outputc_accuracy: 0.8545 - val_outputb_accuracy: 0.9863\n",
            "Epoch 2/10\n",
            "1688/1688 - 11s - loss: 0.3981 - outputc_loss: 0.3473 - outputb_loss: 0.0339 - outputc_accuracy: 0.8757 - outputb_accuracy: 0.9902 - val_loss: 0.3581 - val_outputc_loss: 0.3062 - val_outputb_loss: 0.0346 - val_outputc_accuracy: 0.8920 - val_outputb_accuracy: 0.9897\n",
            "Epoch 3/10\n",
            "1688/1688 - 11s - loss: 0.3332 - outputc_loss: 0.2940 - outputb_loss: 0.0262 - outputc_accuracy: 0.8929 - outputb_accuracy: 0.9919 - val_loss: 0.3179 - val_outputc_loss: 0.2828 - val_outputb_loss: 0.0234 - val_outputc_accuracy: 0.8935 - val_outputb_accuracy: 0.9928\n",
            "Epoch 4/10\n",
            "1688/1688 - 11s - loss: 0.2965 - outputc_loss: 0.2639 - outputb_loss: 0.0218 - outputc_accuracy: 0.9029 - outputb_accuracy: 0.9934 - val_loss: 0.2968 - val_outputc_loss: 0.2663 - val_outputb_loss: 0.0203 - val_outputc_accuracy: 0.9010 - val_outputb_accuracy: 0.9925\n",
            "Epoch 5/10\n",
            "1688/1688 - 11s - loss: 0.2721 - outputc_loss: 0.2442 - outputb_loss: 0.0186 - outputc_accuracy: 0.9116 - outputb_accuracy: 0.9941 - val_loss: 0.3001 - val_outputc_loss: 0.2694 - val_outputb_loss: 0.0205 - val_outputc_accuracy: 0.8995 - val_outputb_accuracy: 0.9928\n",
            "Epoch 6/10\n",
            "1688/1688 - 11s - loss: 0.2510 - outputc_loss: 0.2272 - outputb_loss: 0.0159 - outputc_accuracy: 0.9175 - outputb_accuracy: 0.9946 - val_loss: 0.3186 - val_outputc_loss: 0.2840 - val_outputb_loss: 0.0231 - val_outputc_accuracy: 0.8967 - val_outputb_accuracy: 0.9932\n",
            "Epoch 7/10\n",
            "1688/1688 - 11s - loss: 0.2330 - outputc_loss: 0.2124 - outputb_loss: 0.0138 - outputc_accuracy: 0.9241 - outputb_accuracy: 0.9953 - val_loss: 0.2825 - val_outputc_loss: 0.2562 - val_outputb_loss: 0.0175 - val_outputc_accuracy: 0.9095 - val_outputb_accuracy: 0.9943\n",
            "Epoch 8/10\n",
            "1688/1688 - 11s - loss: 0.2196 - outputc_loss: 0.2017 - outputb_loss: 0.0119 - outputc_accuracy: 0.9272 - outputb_accuracy: 0.9955 - val_loss: 0.2699 - val_outputc_loss: 0.2452 - val_outputb_loss: 0.0165 - val_outputc_accuracy: 0.9130 - val_outputb_accuracy: 0.9950\n",
            "Epoch 9/10\n",
            "1688/1688 - 11s - loss: 0.2056 - outputc_loss: 0.1893 - outputb_loss: 0.0108 - outputc_accuracy: 0.9315 - outputb_accuracy: 0.9967 - val_loss: 0.2561 - val_outputc_loss: 0.2291 - val_outputb_loss: 0.0181 - val_outputc_accuracy: 0.9193 - val_outputb_accuracy: 0.9933\n",
            "Epoch 10/10\n",
            "1688/1688 - 11s - loss: 0.1944 - outputc_loss: 0.1810 - outputb_loss: 0.0090 - outputc_accuracy: 0.9350 - outputb_accuracy: 0.9969 - val_loss: 0.2558 - val_outputc_loss: 0.2345 - val_outputb_loss: 0.0142 - val_outputc_accuracy: 0.9163 - val_outputb_accuracy: 0.9955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpDZ9X6UFnVh",
        "outputId": "00beeb47-3a54-4cb9-f2ff-5022fe232f25"
      },
      "source": [
        "'''\r\n",
        "    TESTING CELL\r\n",
        "'''\r\n",
        "ftmodel=fine_tune(model, n_classes=11, cl_weight=1, b_weight=1, lr=0.001)\r\n",
        "print(model.summary())"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_44\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_layer_common (InputLayer) [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_c (Conv2D)                (None, 28, 28, 64)   640         input_layer_common[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "relu1_c (LeakyReLU)             (None, 28, 28, 64)   0           conv1_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2_c (Conv2D)                (None, 28, 28, 64)   36928       relu1_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "relu2_c (LeakyReLU)             (None, 28, 28, 64)   0           conv2_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool1_c (MaxPooling2D)          (None, 14, 14, 64)   0           relu2_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_c (Conv2D)                (None, 14, 14, 32)   18464       pool1_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_b (Conv2D)                (None, 28, 28, 32)   320         input_layer_common[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "relu3_c (LeakyReLU)             (None, 14, 14, 32)   0           conv3_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "relu1_b (LeakyReLU)             (None, 28, 28, 32)   0           conv1_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv4_c (Conv2D)                (None, 14, 14, 32)   9248        relu3_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2_b (Conv2D)                (None, 28, 28, 32)   9248        relu1_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "relu4_c (LeakyReLU)             (None, 14, 14, 32)   0           conv4_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "relu2_b (LeakyReLU)             (None, 28, 28, 32)   0           conv2_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool2_c (MaxPooling2D)          (None, 7, 7, 32)     0           relu4_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool1_b (MaxPooling2D)          (None, 14, 14, 32)   0           relu2_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv5_c (Conv2D)                (None, 7, 7, 16)     4624        pool2_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_b (Conv2D)                (None, 14, 14, 16)   4624        pool1_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "relu5_c (LeakyReLU)             (None, 7, 7, 16)     0           conv5_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "relu3_b (LeakyReLU)             (None, 14, 14, 16)   0           conv3_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv6_c (Conv2D)                (None, 7, 7, 16)     2320        relu5_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv4_b (Conv2D)                (None, 14, 14, 16)   2320        relu3_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "relu6_c (LeakyReLU)             (None, 7, 7, 16)     0           conv6_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "relu4_b (LeakyReLU)             (None, 14, 14, 16)   0           conv4_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool3_c (MaxPooling2D)          (None, 3, 3, 16)     0           relu6_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool2_b (MaxPooling2D)          (None, 7, 7, 16)     0           relu4_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_c (Flatten)             (None, 144)          0           pool3_c[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_b (Flatten)             (None, 784)          0           pool2_b[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "outputc (Dense)                 (None, 10)           1450        flatten_c[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "outputb (Dense)                 (None, 1)            785         flatten_b[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 90,971\n",
            "Trainable params: 6,875\n",
            "Non-trainable params: 84,096\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}