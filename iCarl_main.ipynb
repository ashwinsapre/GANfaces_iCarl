{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "iCarl_main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinsapre/GANfaces_iCarl/blob/main/iCarl_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y73F_4X-AuAy"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import keras.layers as L\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.applications import DenseNet121\n",
        "from keras.applications import VGG16\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3d2b1v4Nik8",
        "outputId": "069a5dae-ee0c-41cd-c307-c37c207eb81c"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VttqVxphAuA6"
      },
      "source": [
        " For multi-task learning, branching NN out\n",
        "- one branch predicts the class of FashionMNIST object (classification)\n",
        "- other branch predicts whether the object is a \"top\" or not (0/1 output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8-eX21QAuA6"
      },
      "source": [
        "def create_model(n_classes, input_dim, cl_weight, b_weight, lr):\n",
        "    '''\n",
        "        Creating categorical classification model\n",
        "    '''\n",
        "    \n",
        "    inputs=L.Input((input_dim,input_dim,1))\n",
        "    \n",
        "    xc=L.Conv2D(64, kernel_size=3, padding='same', strides=1)(inputs)\n",
        "    xc=L.LeakyReLU(0.2)(xc)\n",
        "    xc=L.Conv2D(64, kernel_size=3, padding='same', strides=1)(xc)\n",
        "    xc=L.LeakyReLU(0.2)(xc)\n",
        "    xc=L.MaxPool2D(pool_size=2, strides=2)(xc)\n",
        "    \n",
        "    xc=L.Conv2D(32, kernel_size=3, padding='same', strides=1)(inputs)\n",
        "    xc=L.LeakyReLU(0.2)(xc)\n",
        "    xc=L.Conv2D(32, kernel_size=3, padding='same', strides=1)(xc)\n",
        "    xc=L.LeakyReLU(0.2)(xc)\n",
        "    xc=L.MaxPool2D(pool_size=2, strides=2)(xc)\n",
        "    \n",
        "    xc=L.Conv2D(16, kernel_size=3, padding='same', strides=1)(xc)\n",
        "    xc=L.LeakyReLU(0.2)(xc)\n",
        "    xc=L.Conv2D(16, kernel_size=3, padding='same', strides=1)(xc)\n",
        "    xc=L.LeakyReLU(0.2)(xc)\n",
        "    xc=L.MaxPool2D(pool_size=2, strides=2)(xc)\n",
        "    \n",
        "    xc=L.Flatten()(xc)\n",
        "    xc=L.Dense(n_classes)(xc)\n",
        "    outputc=L.Softmax(name='outputc')(xc)\n",
        "    \n",
        "    '''\n",
        "        Creating binary classification model (a top/not a top)\n",
        "    '''\n",
        "    \n",
        "    x=L.LeakyReLU(0.2)(inputs)\n",
        "    x=L.Conv2D(32, kernel_size=3, padding='same', strides=1)(x)\n",
        "    x=L.LeakyReLU(0.2)(x)\n",
        "    x=L.MaxPool2D(pool_size=2, strides=2)(x)\n",
        "    \n",
        "    x=L.Conv2D(16, kernel_size=3, padding='same', strides=1)(x)\n",
        "    x=L.LeakyReLU(0.2)(x)\n",
        "    x=L.Conv2D(16, kernel_size=3, padding='same', strides=1)(x)\n",
        "    x=L.LeakyReLU(0.2)(x)\n",
        "    x=L.MaxPool2D(pool_size=2, strides=2)(x)\n",
        "    \n",
        "    x=L.Flatten()(x)\n",
        "    outputb=L.Dense(1, activation='sigmoid', name='outputb')(x)\n",
        "    #outputb=L.Activation('sigmoid', name=\"outputb\")(x)\n",
        "\n",
        "    '''\n",
        "        Combining both models\n",
        "    '''\n",
        "    model=keras.Model(inputs=inputs, outputs=[outputc, outputb])\n",
        "    losses={'outputc':'categorical_crossentropy', 'outputb':'binary_crossentropy'}\n",
        "    loss_weights=[cl_weight, b_weight]\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr), \n",
        "                      loss=losses,\n",
        "                      loss_weights=loss_weights,\n",
        "                      metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3b09_gxAuA7"
      },
      "source": [
        "def train_model(model, n_epochs:int, x_train, y_trainc, y_trainb, validation_split):\n",
        "    history=model.fit(x=x_train, \n",
        "                      y={'outputc':y_trainc, 'outputb': y_trainb}, \n",
        "                      epochs=n_epochs, \n",
        "                      validation_split=0.1,\n",
        "                      shuffle=True,\n",
        "                      verbose=2)\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd5OfDiIAuA7"
      },
      "source": [
        "def calc_feature_vectors(model, images):\n",
        "    #extract feature vector from a layer of ALREADY-TRAINED model\n",
        "    #size of feature vector=784\n",
        "    feature_model=keras.Model(inputs=model.inputs, outputs=model.get_layer(\"flatten\").output)\n",
        "    feature_vectors=feature_model.predict(images)\n",
        "    norms=np.linalg.norm(feature_vectors, keepdims=True, axis=1)\n",
        "    feature_vectors=feature_vectors/norms\n",
        "    return feature_vectors"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jymSn4mRtO5H"
      },
      "source": [
        "def create_exemplar_set(mem_size, n_classes, feature_vectors, labels, reconstruct=False):\r\n",
        "    per_class=mem_size//n_classes\r\n",
        "\r\n",
        "    class_vectors={}\r\n",
        "    mean_class_vectors={}\r\n",
        "    class_vectors_distances={}\r\n",
        "\r\n",
        "    #init dicts\r\n",
        "    for i in range(n_classes):\r\n",
        "      class_vectors[i]=[]\r\n",
        "      mean_class_vectors[i]=[]\r\n",
        "      class_vectors_distances[i]=[]\r\n",
        "    \r\n",
        "    #vectors belonging to class i go in the list in key i of class_vectors\r\n",
        "    for i in range(len(labels)):\r\n",
        "      class_vectors[labels[i]].append(feature_vectors[i])\r\n",
        "\r\n",
        "    #calculate mean class vectors by summing all vectors per class and diving by length\r\n",
        "    for i in range(n_classes):\r\n",
        "      mean_class_vectors[i]=np.sum(class_vectors[i], axis=0)/len(class_vectors[i])\r\n",
        "    \r\n",
        "    #calculating distances from mean\r\n",
        "    for i in range(len(labels)):\r\n",
        "      class_vectors_distances[labels[i]].append(np.linalg.norm(mean_class_vectors[labels[i]] - feature_vectors[i]))\r\n",
        "\r\n",
        "    #sorting vectors by their corresponding distances from class means\r\n",
        "    for i in range(n_classes):\r\n",
        "      class_vectors_distances[i], class_vectors[i] = (list(t) for t in zip(*sorted(zip(class_vectors_distances[i], class_vectors[i]))))\r\n",
        "\r\n",
        "    exemplars_x = []\r\n",
        "    exemplars_y = []\r\n",
        "\r\n",
        "    #choose 'per_class' number of vectors\r\n",
        "    for i in range(n_classes):\r\n",
        "      exemplars_x.append(class_vectors[i][:per_class])\r\n",
        "      exemplars_y+= per_class*[i]\r\n",
        "\r\n",
        "    return exemplars_x, exemplars_y"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Wu1QlhAuA7"
      },
      "source": [
        "def classify(X, mean_class_vectors, n_classes):\n",
        "    #image embedding created\n",
        "    image_vector=calc_feature_vectors(model, X)\n",
        "    #distance to all mean vectors calculated\n",
        "    distances=[]\n",
        "    indices=[x for x in range(n_classes)]\n",
        "    #sort class values by their corresponding distances\n",
        "    for i in range(n_classes):\n",
        "      distances.append(np.linalg.norm(mean_class_vectors[i] - image_vector))\n",
        "    distances, indices = (list(t) for t in zip(*sorted(zip(distances, indices))))\n",
        "    #return the class with the least distance\n",
        "    return indices[0]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l48ckkJAuA8"
      },
      "source": [
        "def transform_outputs(y_train, y_test):\n",
        "    labeldict={}\n",
        "    labeldict[0]=1\n",
        "    labeldict[1]=0\n",
        "    labeldict[2]=1\n",
        "    labeldict[3]=1\n",
        "    labeldict[4]=1\n",
        "    labeldict[5]=0\n",
        "    labeldict[6]=1\n",
        "    labeldict[7]=0\n",
        "    labeldict[8]=0\n",
        "    labeldict[9]=0\n",
        "    \n",
        "    y_trainb=[]\n",
        "    y_testb=[]\n",
        "    y_trainc=keras.utils.to_categorical(y_train)\n",
        "    y_testc=keras.utils.to_categorical(y_test)\n",
        "    \n",
        "    for i in range(len(y_train)):\n",
        "        y_trainb.append(labeldict[y_train[i]])\n",
        "        \n",
        "    for i in range(len(y_test)):\n",
        "        y_testb.append(labeldict[y_test[i]])\n",
        "    \n",
        "    return y_trainc, y_testc, np.asarray(y_trainb), np.asarray(y_testb)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwrcNzQZAuA8"
      },
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train=x_train.reshape((x_train.shape[0], 28,28,1))\n",
        "x_test=x_test.reshape((x_test.shape[0],28,28,1))\n",
        "y_trainc, y_testc, y_trainb, y_testb=transform_outputs(y_train, y_test)\n",
        "x_train=x_train/255.0\n",
        "x_test=x_test/255.0\n",
        "\n",
        "#np.savetxt(\"ytrain_transformed.csv\", y_train, delimiter=',')\n",
        "#np.savetxt(\"ytest_transformed.csv\", y_test, delimiter=',')\n",
        "    \n",
        "#y_train=np.loadtxt(open(\"ytrain_transformed.csv\", \"rb\"), delimiter=',')\n",
        "#y_test=np.loadtxt(open(\"ytest_transformed.csv\", \"rb\"), delimiter=',')\n",
        "    \n",
        "model=create_model(n_classes=10, input_dim=28, cl_weight=1.0, b_weight=1.5, lr=0.0005)\n",
        "    \n",
        "train_model(model, 50, x_train, y_trainc, y_trainb, validation_split=0.1)\n",
        "model.save(\"trainedmodel.h5\")\n",
        "#-----------------------------------------------------------------------#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "tpDZ9X6UFnVh",
        "outputId": "c1e95f0a-11e9-4eaf-c7e9-30031e6946d3"
      },
      "source": [
        "'''\r\n",
        "    TESTING CELL\r\n",
        "'''\r\n",
        "x=model.layers[-3].output\r\n",
        "x=L.Dense(11)(x)\r\n",
        "new_outputc=L.Softmax(name='new_outputc')(x)\r\n",
        "new_model=keras.Model(inputs=model.inputs,\r\n",
        "                      outputs=[new_outputc, outputb])\r\n",
        "print(new_model.summary())\r\n",
        "losses={'outputc':'categorical_crossentropy', 'outputb':'binary_crossentropy'}\r\n",
        "#loss_weights=[cl_weight, b_weight]\r\n",
        "#model.compile(optimizer=keras.optimizers.Adam(lr), loss=losses, loss_weights=loss_weights, metrics=['accuracy'])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e45ff1228a27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnew_outputc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'new_outputc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m new_model=keras.Model(inputs=model.inputs,\n\u001b[0;32m----> 8\u001b[0;31m                       outputs=[new_outputc, outputb])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'outputc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outputb'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'outputb' is not defined"
          ]
        }
      ]
    }
  ]
}